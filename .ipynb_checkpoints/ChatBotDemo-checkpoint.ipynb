{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##A simple chatbot build a BOW model from Wikipedia page\n",
    "#Answer simple questions about Turkey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utf-8\n",
      "UTF-8\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup  \n",
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "import os, sys\n",
    "\n",
    "print(sys.getdefaultencoding())\n",
    "print(sys.stdout.encoding)\n",
    "\n",
    "from os.path import join\n",
    "\n",
    "#for encoding problems\n",
    "import ftfy\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string # to process standard python strings\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "url = \"http://www.wikizero.biz/index.php?q=aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvVHVya2V5\"\n",
    "resp = requests.get(url)\n",
    "print(resp.status_code)\n",
    "soup = BeautifulSoup(resp.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_content = soup.findAll(\"p\")\n",
    "#raw_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getText(raw_content):\n",
    "    \n",
    "    document = []\n",
    "    \n",
    "    for par in raw_content:\n",
    "        if par.text != \"\" and len(par.text) > 3:\n",
    "            document.append(par.text)\n",
    "    \n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regex = re.findall(\"...]\",raw_text)\n",
    "#\n",
    "#pattern = \"....]\"\n",
    "#for match in re.finditer(pattern, raw_text):\n",
    "#    s = match.start()\n",
    "#    e = match.end()\n",
    "#    print(raw_text[s:e])\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = getText(raw_content)\n",
    "raw_text = ''.join(doc)\n",
    "to_be_removed = []\n",
    "for i in range(499):\n",
    "    str_ = '['+str(i)+']'\n",
    "    to_be_removed.append(str_)\n",
    "for each in to_be_removed:\n",
    "    raw_text = raw_text.replace(each,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt') # first-time use only\n",
    "#nltk.download('wordnet') # first-time use only\n",
    "sent_tokens = nltk.sent_tokenize(raw_text)# converts to list of sentences \n",
    "word_tokens = nltk.word_tokenize(raw_text)# converts to list of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TfidfVec = TfidfVectorizer(tokenizer=LemNormalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    robo_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    sent_tokens.pop(-1)\n",
    "\n",
    "    flat = vals.flatten()  \n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    \n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+\"I am sorry! I don't understand you\"\n",
    "        return robo_response\n",
    "    \n",
    "    for i in range(5):\n",
    "        idx=vals.argsort()[0][-i-2]\n",
    "        robo_response = robo_response + sent_tokens[idx] +  \" \"\n",
    "\n",
    "    return robo_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBot: I will answer you questions about Turkey What do you want to learn? \n",
      "\n",
      "hello\n",
      "RoBot: hi there\n",
      "\n",
      "nasılsın\n",
      "RoBot: I am sorry! I don't understand you\n",
      "\n",
      "turkey's culture\n",
      "RoBot: Turkey has a very diverse culture that is a blend of various elements of the Turkic, Anatolian, Ottoman (which was itself a continuation of both Greco-Roman and Islamic cultures) and Western culture and traditions, which started with the Westernisation of the Ottoman Empire and still continues today. The process of Hellenization that began with Alexander's conquest accelerated under Roman rule, and by the early centuries of the Christian Era, the local Anatolian languages and cultures had become extinct, being largely replaced by ancient Greek language and culture. The Turkish Ministry of Culture and Tourism currently promotes Turkish tourism under the Turkey Home name. Turkish culture is a product of efforts to be a \"modern\" Western state, while maintaining traditional religious and historical values. Prior to the symposia, the study of Turkish culinary culture was first popularized by the publication of Süheyl Ünver's Fifty Dishes in Turkish History in 1948. \n",
      "\n",
      "turkish economy\n",
      "RoBot: Since the liberalisation of the Turkish economy in the 1980s, the country has enjoyed stronger economic growth and greater political stability. The economy was estimated to have returned to 8 percent growth in 2010. Tourism in Turkey has increased almost every year in the 21st century, and is an important part of the economy. However, growth slowed to 1 percent in 2008, and in 2009 the Turkish economy was affected by the global financial crisis, with a recession of 5 percent. In 1948 both countries were included in the Marshall Plan and the OEEC for rebuilding European economies. \n",
      "\n",
      "exit\n",
      "RoBot: I am sorry! I don't understand you\n",
      "\n",
      "bye\n",
      "RoBot:We wolud be glad to see you in Turkey. There are a lot to discover  ( ͡° ͜ʖ ͡°)\n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"what's up\",\"hey\",)\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"hi there\", \"hello\"]\n",
    "def greeting(sentence):\n",
    " \n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)\n",
    "\n",
    "print(\"RoBot: I will answer you questions about Turkey What do you want to learn? \\n\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    \n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"RoBot: You are welcome..\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"RoBot: \"+greeting(user_response)+\"\\n\")\n",
    "            else:\n",
    "                print(\"RoBot: \"+response(user_response)+\"\\n\")\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"RoBot:We wolud be glad to see you in Turkey. There are a lot to discover  ( ͡° ͜ʖ ͡°)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
